{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa7e0a4-42bc-4557-8600-3beab261ff1c",
   "metadata": {},
   "source": [
    "- Coding a neural network using only matrix multiplication\n",
    "- Writting a dataloader to be used in training\n",
    "- Creating a NN with high level classes and sequential api\n",
    "- Using an optimizer and pre-defined loss\n",
    "- Using tf and keras\n",
    "\n",
    "Data: \n",
    "- [mnist_train](https://drive.google.com/file/d/1bCVtZBPQcEz3AqkvrI1M69D-mcY6f5TK/view?usp=sharing)\n",
    "- [mnist_test](https://drive.google.com/file/d/1DrN5-afU-U6x5hMrUgpUaOA7wYZzXkmz/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e484f5-6f44-447d-885c-d91687b24087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2317f2-b117-474f-a528-7f3053b0a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path_mnist_train = \"/Users/gunnvantsaini/OneDrive/project_codes/content/dl_basics/vision/sony/data/mnist_train.csv\"\n",
    "local_path_mnist_test = \"/Users/gunnvantsaini/OneDrive/project_codes/content/dl_basics/vision/sony/data/mnist_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73b2cf-bc50-4316-97e3-198af2708e94",
   "metadata": {},
   "source": [
    "### Coding a neural network using only matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00641af5-fb6c-4ac8-b065-1f770dab02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv(local_path_mnist_train)\n",
    "mnist_test = pd.read_csv(local_path_mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d81f98-f812-4530-929d-237f54c277f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b191fe5-72f0-4683-aafd-4a572873a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X=torch.tensor(X,dtype=torch.float)\n",
    "y=torch.tensor(y,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a7aa24-315d-4ca2-9294-a1a2eea2a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=torch.randn((784,3),dtype=torch.float)\n",
    "b1=torch.randn((3,),dtype=torch.float)\n",
    "w2=torch.randn((3,10),dtype=torch.float)\n",
    "b2=torch.randn((10,),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ec9a6c-f0a4-48f1-b477-137bb677c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(X,w1,b1,w2,b2):\n",
    "    z1=torch.matmul(X.float(),w1)+b1\n",
    "    res1=torch.sigmoid(z1)\n",
    "    z2=torch.matmul(res1,w2)+b2\n",
    "    probs=torch.softmax(z2,axis=1)\n",
    "    return probs\n",
    "def CE(probs,y):\n",
    "    return -torch.log(probs[range(y.shape[0]),y.long()]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c4a82d-8230-4063-b913-da6945863a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward pass\n",
    "p=network(X,w1,b1,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87ba4ce-8235-4549-960b-94d4ddc5f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2862)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loss\n",
    "CE(p,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f17e25-d7b9-42bf-8183-3caa80b8185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, loss 3.1359598636627197, acc 0.09988094866275787\n",
      "Iter 2, loss 3.091695547103882, acc 0.100095234811306\n",
      "Iter 3, loss 3.0513336658477783, acc 0.10035714507102966\n",
      "Iter 4, loss 3.0144829750061035, acc 0.10061904788017273\n",
      "Iter 5, loss 2.9807841777801514, acc 0.10104762017726898\n",
      "Iter 6, loss 2.9499104022979736, acc 0.1014999970793724\n",
      "Iter 7, loss 2.921565532684326, acc 0.10180952399969101\n",
      "Iter 8, loss 2.8954837322235107, acc 0.1022142842411995\n",
      "Iter 9, loss 2.8714263439178467, acc 0.10264285653829575\n",
      "Iter 10, loss 2.8491806983947754, acc 0.103071428835392\n"
     ]
    }
   ],
   "source": [
    "## training loop\n",
    "w1=torch.randn((784,3),dtype=torch.float,requires_grad=True)\n",
    "b1=torch.randn((3,),dtype=torch.float,requires_grad=True)\n",
    "w2=torch.randn((3,10),dtype=torch.float,requires_grad=True)\n",
    "b2=torch.randn((10,),dtype=torch.float,requires_grad=True)\n",
    "lr=0.1\n",
    "Loss=[]\n",
    "for i in range(10):\n",
    "    p=network(X,w1,b1,w2,b2)\n",
    "    #print(p)\n",
    "    loss=CE(p,y)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.item())\n",
    "    acc=(p.argmax(axis=1)==y).float().mean().item()\n",
    "    print(f\"Iter {i+1}, loss {loss.item()}, acc {acc}\")\n",
    "    with torch.no_grad():\n",
    "        w1-=lr*w1.grad\n",
    "        b1-=lr*b1.grad\n",
    "        w2-=lr*w2.grad\n",
    "        b2-=lr*b2.grad\n",
    "        w1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        b2.grad.zero_()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26a0299-3750-4a20-b9e0-287199681657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistData(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        X=self.X[idx,]\n",
    "        y=self.y[idx]\n",
    "        sample={'X':X,'y':y}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b60278-914b-4ba3-9828-132001559275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "mnist=MnistData(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8750258b-dc35-4968-b7b7-d0f9d9501601",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_batched=DataLoader(mnist,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965889d1-aadc-4334-905a-b8903e8b9dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " 'y': tensor([1, 0, 1, 4, 0, 0, 7, 3, 5, 3, 8, 9, 1, 3, 3, 1, 2, 0, 7, 5, 8, 6, 2, 0,\n",
       "         2, 3, 6, 9, 9, 7, 8, 9, 4, 9, 2, 1, 3, 1, 1, 4, 9, 1, 4, 4, 2, 6, 3, 7,\n",
       "         7, 4, 7, 5, 1, 9, 0, 2, 2, 3, 9, 1, 1, 1, 5, 0, 6, 3, 4, 8, 1, 0, 3, 9,\n",
       "         6, 2, 6, 4, 7, 1, 4, 1, 5, 4, 8, 9, 2, 9, 9, 8, 9, 6, 3, 6, 4, 6, 2, 9,\n",
       "         1, 2, 0, 5])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(mnist_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46195e30-4bfe-47b1-b444-bd0d1c4e7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class Excercise use the regression.csv and create a Dataloader using that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd96546b-7961-49da-ab79-470be72dcf21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegData(Dataset):\n",
    "    def __init__(self,path,y_name):\n",
    "        self.path = path\n",
    "        self.y_name = y_name\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.X = self.data.drop(self.y_name,axis=1).values\n",
    "        self.y = self.data[self.y_name].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return {'X':x,'y':y}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c14e7f-96b6-4a2a-9a2c-689d8703fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = RegData(path=\"../data/regression.csv\",y_name='mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59c4b671-4b0f-42e9-a8e8-89a6b43db53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = DataLoader(d,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ccf83e3-9fd5-46c7-860b-69d9e5796d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[8.0000e+00, 3.0700e+02, 1.3000e+02, 3.5040e+03, 1.2000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.5000e+02, 1.6500e+02, 3.6930e+03, 1.1500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.1800e+02, 1.5000e+02, 3.4360e+03, 1.1000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0400e+02, 1.5000e+02, 3.4330e+03, 1.2000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0200e+02, 1.4000e+02, 3.4490e+03, 1.0500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.2900e+02, 1.9800e+02, 4.3410e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.5400e+02, 2.2000e+02, 4.3540e+03, 9.0000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.4000e+02, 2.1500e+02, 4.3120e+03, 8.5000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.5500e+02, 2.2500e+02, 4.4250e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.9000e+02, 1.9000e+02, 3.8500e+03, 8.5000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.8300e+02, 1.7000e+02, 3.5630e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.4000e+02, 1.6000e+02, 3.6090e+03, 8.0000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.0000e+02, 1.5000e+02, 3.7610e+03, 9.5000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.5500e+02, 2.2500e+02, 3.0860e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 1.1300e+02, 9.5000e+01, 2.3720e+03, 1.5000e+01, 7.0000e+01,\n",
       "          3.0000e+00],\n",
       "         [6.0000e+00, 1.9800e+02, 9.5000e+01, 2.8330e+03, 1.5500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [6.0000e+00, 1.9900e+02, 9.7000e+01, 2.7740e+03, 1.5500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [6.0000e+00, 2.0000e+02, 8.5000e+01, 2.5870e+03, 1.6000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 9.7000e+01, 8.8000e+01, 2.1300e+03, 1.4500e+01, 7.0000e+01,\n",
       "          3.0000e+00],\n",
       "         [4.0000e+00, 9.7000e+01, 4.6000e+01, 1.8350e+03, 2.0500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.1000e+02, 8.7000e+01, 2.6720e+03, 1.7500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.0700e+02, 9.0000e+01, 2.4300e+03, 1.4500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.0400e+02, 9.5000e+01, 2.3750e+03, 1.7500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.2100e+02, 1.1300e+02, 2.2340e+03, 1.2500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [6.0000e+00, 1.9900e+02, 9.0000e+01, 2.6480e+03, 1.5000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.6000e+02, 2.1500e+02, 4.6150e+03, 1.4000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0700e+02, 2.0000e+02, 4.3760e+03, 1.5000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.1800e+02, 2.1000e+02, 4.3820e+03, 1.3500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0400e+02, 1.9300e+02, 4.7320e+03, 1.8500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 9.7000e+01, 8.8000e+01, 2.1300e+03, 1.4500e+01, 7.1000e+01,\n",
       "          3.0000e+00],\n",
       "         [4.0000e+00, 1.4000e+02, 9.0000e+01, 2.2640e+03, 1.5500e+01, 7.1000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 1.1300e+02, 9.5000e+01, 2.2280e+03, 1.4000e+01, 7.1000e+01,\n",
       "          3.0000e+00]], dtype=torch.float64),\n",
       " 'y': tensor([18., 15., 18., 16., 17., 15., 14., 14., 14., 15., 15., 14., 15., 14.,\n",
       "         24., 22., 18., 21., 27., 26., 25., 24., 25., 26., 21., 10., 10., 11.,\n",
       "          9., 27., 28., 25.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(d_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973e4c6-6aca-425c-88e7-b9a41963a21e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training using a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e959c9a-5a11-4d6a-8b32-5201cf95c62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 3.6475841999053955, acc 0.14000000059604645\n",
      "Epoch 1, iter 101, loss 2.959592342376709, acc 0.12999999523162842\n",
      "Epoch 1, iter 201, loss 2.8314015865325928, acc 0.11999999731779099\n",
      "Epoch 1, iter 301, loss 2.713552236557007, acc 0.10999999940395355\n",
      "Epoch 1, iter 401, loss 2.654496669769287, acc 0.09000000357627869\n",
      "Epoch 2, iter 1, loss 2.6646318435668945, acc 0.12999999523162842\n",
      "Epoch 2, iter 101, loss 2.4830801486968994, acc 0.11999999731779099\n",
      "Epoch 2, iter 201, loss 2.4732818603515625, acc 0.10000000149011612\n",
      "Epoch 2, iter 301, loss 2.473522901535034, acc 0.10999999940395355\n",
      "Epoch 2, iter 401, loss 2.4026601314544678, acc 0.10999999940395355\n",
      "Epoch 3, iter 1, loss 2.434114933013916, acc 0.10000000149011612\n",
      "Epoch 3, iter 101, loss 2.3586008548736572, acc 0.10999999940395355\n",
      "Epoch 3, iter 201, loss 2.3317301273345947, acc 0.09000000357627869\n",
      "Epoch 3, iter 301, loss 2.375793933868408, acc 0.10999999940395355\n",
      "Epoch 3, iter 401, loss 2.2883334159851074, acc 0.12999999523162842\n",
      "Epoch 4, iter 1, loss 2.299011707305908, acc 0.15000000596046448\n",
      "Epoch 4, iter 101, loss 2.2841227054595947, acc 0.11999999731779099\n",
      "Epoch 4, iter 201, loss 2.2466039657592773, acc 0.12999999523162842\n",
      "Epoch 4, iter 301, loss 2.3195598125457764, acc 0.12999999523162842\n",
      "Epoch 4, iter 401, loss 2.2278389930725098, acc 0.23000000417232513\n",
      "Epoch 5, iter 1, loss 2.212808609008789, acc 0.17000000178813934\n",
      "Epoch 5, iter 101, loss 2.2272696495056152, acc 0.20000000298023224\n",
      "Epoch 5, iter 201, loss 2.1902005672454834, acc 0.17000000178813934\n",
      "Epoch 5, iter 301, loss 2.2830810546875, acc 0.15000000596046448\n",
      "Epoch 5, iter 401, loss 2.1911191940307617, acc 0.3100000023841858\n",
      "Epoch 6, iter 1, loss 2.1588187217712402, acc 0.20999999344348907\n",
      "Epoch 6, iter 101, loss 2.1807308197021484, acc 0.23000000417232513\n",
      "Epoch 6, iter 201, loss 2.1483194828033447, acc 0.1899999976158142\n",
      "Epoch 6, iter 301, loss 2.2550559043884277, acc 0.17000000178813934\n",
      "Epoch 6, iter 401, loss 2.1641318798065186, acc 0.3100000023841858\n",
      "Epoch 7, iter 1, loss 2.1218972206115723, acc 0.23000000417232513\n",
      "Epoch 7, iter 101, loss 2.1433522701263428, acc 0.23000000417232513\n",
      "Epoch 7, iter 201, loss 2.1137795448303223, acc 0.20000000298023224\n",
      "Epoch 7, iter 301, loss 2.230170488357544, acc 0.17000000178813934\n",
      "Epoch 7, iter 401, loss 2.142632484436035, acc 0.3100000023841858\n",
      "Epoch 8, iter 1, loss 2.092475652694702, acc 0.23000000417232513\n",
      "Epoch 8, iter 101, loss 2.1153855323791504, acc 0.23000000417232513\n",
      "Epoch 8, iter 201, loss 2.084869146347046, acc 0.2199999988079071\n",
      "Epoch 8, iter 301, loss 2.206044912338257, acc 0.18000000715255737\n",
      "Epoch 8, iter 401, loss 2.1249828338623047, acc 0.28999999165534973\n",
      "Epoch 9, iter 1, loss 2.066812038421631, acc 0.23999999463558197\n",
      "Epoch 9, iter 101, loss 2.095336675643921, acc 0.23000000417232513\n",
      "Epoch 9, iter 201, loss 2.0617034435272217, acc 0.23000000417232513\n",
      "Epoch 9, iter 301, loss 2.1819281578063965, acc 0.1899999976158142\n",
      "Epoch 9, iter 401, loss 2.1098151206970215, acc 0.28999999165534973\n",
      "Epoch 10, iter 1, loss 2.043797016143799, acc 0.23999999463558197\n",
      "Epoch 10, iter 101, loss 2.0807175636291504, acc 0.23999999463558197\n",
      "Epoch 10, iter 201, loss 2.043794870376587, acc 0.23999999463558197\n",
      "Epoch 10, iter 301, loss 2.15759015083313, acc 0.20000000298023224\n",
      "Epoch 10, iter 401, loss 2.095715284347534, acc 0.3100000023841858\n"
     ]
    }
   ],
   "source": [
    "w1=torch.randn((784,3),dtype=torch.float,requires_grad=True)\n",
    "b1=torch.randn((3,),dtype=torch.float,requires_grad=True)\n",
    "w2=torch.randn((3,10),dtype=torch.float,requires_grad=True)\n",
    "b2=torch.randn((10,),dtype=torch.float,requires_grad=True)\n",
    "lr=0.01\n",
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=network(x,w1,b1,w2,b2)\n",
    "        loss=CE(p,Y)\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        with torch.no_grad():\n",
    "            w1-=lr*w1.grad\n",
    "            b1-=lr*b1.grad\n",
    "            w2-=lr*w2.grad\n",
    "            b2-=lr*b2.grad\n",
    "            w1.grad.zero_()\n",
    "            b1.grad.zero_()\n",
    "            w2.grad.zero_()\n",
    "            b2.grad.zero_()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb301bfe-87e0-4d50-b399-3a2e97e1374f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Writting a network with a high level api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b20d81b-3bff-4213-b21d-5ff35c123f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b00fc2e3-7051-4ff7-af95-7c9c3960fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a model with nn class ####\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1=nn.Parameter(torch.randn((784,3),dtype=torch.float))\n",
    "        self.b1=nn.Parameter(torch.randn((3,),dtype=torch.float))\n",
    "        self.w2=nn.Parameter(torch.randn((3,10),dtype=torch.float))\n",
    "        self.b2=nn.Parameter(torch.randn((10,),dtype=torch.float))\n",
    "    def forward(self,X):\n",
    "        z1=torch.matmul(X.float(),self.w1)+self.b1\n",
    "        res1=torch.sigmoid(z1)\n",
    "        z2=torch.matmul(res1,self.w2)+self.b2\n",
    "        probs=torch.softmax(z2,axis=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35cc3ead-f4b5-46f7-a49b-d7f803e82e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af2b25a-8208-42fb-9ed1-dbb995df6728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 3.5367236137390137, acc 0.10999999940395355\n",
      "Epoch 1, iter 101, loss 3.0288314819335938, acc 0.07000000029802322\n",
      "Epoch 1, iter 201, loss 2.8319709300994873, acc 0.05000000074505806\n",
      "Epoch 1, iter 301, loss 2.6407525539398193, acc 0.10999999940395355\n",
      "Epoch 1, iter 401, loss 2.3520538806915283, acc 0.10999999940395355\n",
      "Epoch 2, iter 1, loss 2.3813798427581787, acc 0.15000000596046448\n",
      "Epoch 2, iter 101, loss 2.4469709396362305, acc 0.07000000029802322\n",
      "Epoch 2, iter 201, loss 2.4275898933410645, acc 0.11999999731779099\n",
      "Epoch 2, iter 301, loss 2.3922646045684814, acc 0.14000000059604645\n",
      "Epoch 2, iter 401, loss 2.2707107067108154, acc 0.18000000715255737\n",
      "Epoch 3, iter 1, loss 2.259216070175171, acc 0.15000000596046448\n",
      "Epoch 3, iter 101, loss 2.3237292766571045, acc 0.07999999821186066\n",
      "Epoch 3, iter 201, loss 2.2869784832000732, acc 0.09000000357627869\n",
      "Epoch 3, iter 301, loss 2.3006675243377686, acc 0.11999999731779099\n",
      "Epoch 3, iter 401, loss 2.251497983932495, acc 0.1899999976158142\n",
      "Epoch 4, iter 1, loss 2.2269351482391357, acc 0.20999999344348907\n",
      "Epoch 4, iter 101, loss 2.2837820053100586, acc 0.14000000059604645\n",
      "Epoch 4, iter 201, loss 2.231997489929199, acc 0.1599999964237213\n",
      "Epoch 4, iter 301, loss 2.2611043453216553, acc 0.18000000715255737\n",
      "Epoch 4, iter 401, loss 2.239938735961914, acc 0.20999999344348907\n",
      "Epoch 5, iter 1, loss 2.2116613388061523, acc 0.18000000715255737\n",
      "Epoch 5, iter 101, loss 2.26503324508667, acc 0.1599999964237213\n",
      "Epoch 5, iter 201, loss 2.2022249698638916, acc 0.23999999463558197\n",
      "Epoch 5, iter 301, loss 2.2361538410186768, acc 0.1899999976158142\n",
      "Epoch 5, iter 401, loss 2.224877119064331, acc 0.23000000417232513\n",
      "Epoch 6, iter 1, loss 2.1969258785247803, acc 0.20999999344348907\n",
      "Epoch 6, iter 101, loss 2.2498972415924072, acc 0.18000000715255737\n",
      "Epoch 6, iter 201, loss 2.1772379875183105, acc 0.25999999046325684\n",
      "Epoch 6, iter 301, loss 2.2142975330352783, acc 0.20999999344348907\n",
      "Epoch 6, iter 401, loss 2.2063963413238525, acc 0.20999999344348907\n",
      "Epoch 7, iter 1, loss 2.1812288761138916, acc 0.23000000417232513\n",
      "Epoch 7, iter 101, loss 2.2335829734802246, acc 0.20000000298023224\n",
      "Epoch 7, iter 201, loss 2.1511638164520264, acc 0.28999999165534973\n",
      "Epoch 7, iter 301, loss 2.1907527446746826, acc 0.2199999988079071\n",
      "Epoch 7, iter 401, loss 2.1852447986602783, acc 0.2199999988079071\n",
      "Epoch 8, iter 1, loss 2.164668560028076, acc 0.23000000417232513\n",
      "Epoch 8, iter 101, loss 2.2138113975524902, acc 0.20000000298023224\n",
      "Epoch 8, iter 201, loss 2.123547315597534, acc 0.2800000011920929\n",
      "Epoch 8, iter 301, loss 2.162490129470825, acc 0.2199999988079071\n",
      "Epoch 8, iter 401, loss 2.162322521209717, acc 0.2199999988079071\n",
      "Epoch 9, iter 1, loss 2.147339105606079, acc 0.23000000417232513\n",
      "Epoch 9, iter 101, loss 2.189784049987793, acc 0.2199999988079071\n",
      "Epoch 9, iter 201, loss 2.0964040756225586, acc 0.30000001192092896\n",
      "Epoch 9, iter 301, loss 2.1305572986602783, acc 0.25999999046325684\n",
      "Epoch 9, iter 401, loss 2.139387845993042, acc 0.23000000417232513\n",
      "Epoch 10, iter 1, loss 2.1294820308685303, acc 0.25999999046325684\n",
      "Epoch 10, iter 101, loss 2.1621315479278564, acc 0.20999999344348907\n",
      "Epoch 10, iter 201, loss 2.0708465576171875, acc 0.30000001192092896\n",
      "Epoch 10, iter 301, loss 2.0989110469818115, acc 0.25\n",
      "Epoch 10, iter 401, loss 2.1180472373962402, acc 0.20999999344348907\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=mod(x)\n",
    "        loss=CE(p,Y)\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        with torch.no_grad():\n",
    "            for p in mod.parameters():\n",
    "                p-=lr*p.grad\n",
    "            mod.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75e9f450-66b4-4526-8e60-ab8606d15a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Can we improve this further, should we be declaring parameters? Shouldn't there be abstractions for layers?\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1=nn.Linear(784,3)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.lin2=nn.Linear(3,10)\n",
    "        self.softmax=nn.Softmax()\n",
    "    def forward(self,X):\n",
    "        x=self.lin1(X)\n",
    "        x=self.sig(x)\n",
    "        x=self.lin2(x)\n",
    "        x=self.softmax(x)\n",
    "        return x           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce771811-241c-4d6c-9a2c-fd9f9a939e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aeefcb47-543f-48fc-ab3c-48cd69c08fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9dbdfe1-6089-4083-8915-0ed2f4c92758",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optim.SGD(mod.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88fb4fe6-057b-454d-9f19-81f3a0fcf2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7h/01sjg2sx6tl3y72r2klm24c80000gn/T/ipykernel_13130/2704172253.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 2.372739791870117, acc 0.05999999865889549\n",
      "Epoch 1, iter 101, loss 2.1189982891082764, acc 0.3400000035762787\n",
      "Epoch 1, iter 201, loss 1.9030648469924927, acc 0.550000011920929\n",
      "Epoch 1, iter 301, loss 1.7667666673660278, acc 0.5799999833106995\n",
      "Epoch 1, iter 401, loss 1.71932053565979, acc 0.4699999988079071\n",
      "Epoch 2, iter 1, loss 1.6427087783813477, acc 0.5799999833106995\n",
      "Epoch 2, iter 101, loss 1.5365495681762695, acc 0.5600000023841858\n",
      "Epoch 2, iter 201, loss 1.3791559934616089, acc 0.6600000262260437\n",
      "Epoch 2, iter 301, loss 1.3651549816131592, acc 0.5600000023841858\n",
      "Epoch 2, iter 401, loss 1.4177724123001099, acc 0.5699999928474426\n",
      "Epoch 3, iter 1, loss 1.3656424283981323, acc 0.5899999737739563\n",
      "Epoch 3, iter 101, loss 1.2652097940444946, acc 0.6200000047683716\n",
      "Epoch 3, iter 201, loss 1.149062991142273, acc 0.699999988079071\n",
      "Epoch 3, iter 301, loss 1.165700078010559, acc 0.6299999952316284\n",
      "Epoch 3, iter 401, loss 1.249300241470337, acc 0.6299999952316284\n",
      "Epoch 4, iter 1, loss 1.2222832441329956, acc 0.5899999737739563\n",
      "Epoch 4, iter 101, loss 1.1125510931015015, acc 0.6100000143051147\n",
      "Epoch 4, iter 201, loss 1.0214030742645264, acc 0.699999988079071\n",
      "Epoch 4, iter 301, loss 1.0475554466247559, acc 0.6600000262260437\n",
      "Epoch 4, iter 401, loss 1.1494646072387695, acc 0.6499999761581421\n",
      "Epoch 5, iter 1, loss 1.133814811706543, acc 0.6100000143051147\n",
      "Epoch 5, iter 101, loss 1.0190790891647339, acc 0.6200000047683716\n",
      "Epoch 5, iter 201, loss 0.9467416405677795, acc 0.7300000190734863\n",
      "Epoch 5, iter 301, loss 0.9777512550354004, acc 0.6800000071525574\n",
      "Epoch 5, iter 401, loss 1.0886332988739014, acc 0.6700000166893005\n",
      "Epoch 6, iter 1, loss 1.0765266418457031, acc 0.6499999761581421\n",
      "Epoch 6, iter 101, loss 0.9573004245758057, acc 0.6499999761581421\n",
      "Epoch 6, iter 201, loss 0.8988282680511475, acc 0.7300000190734863\n",
      "Epoch 6, iter 301, loss 0.9344449043273926, acc 0.6800000071525574\n",
      "Epoch 6, iter 401, loss 1.0482776165008545, acc 0.6800000071525574\n",
      "Epoch 7, iter 1, loss 1.0379284620285034, acc 0.6600000262260437\n",
      "Epoch 7, iter 101, loss 0.9142851233482361, acc 0.6600000262260437\n",
      "Epoch 7, iter 201, loss 0.8650866746902466, acc 0.7200000286102295\n",
      "Epoch 7, iter 301, loss 0.9050601124763489, acc 0.6899999976158142\n",
      "Epoch 7, iter 401, loss 1.0195955038070679, acc 0.6899999976158142\n",
      "Epoch 8, iter 1, loss 1.0101321935653687, acc 0.6700000166893005\n",
      "Epoch 8, iter 101, loss 0.8832756280899048, acc 0.6700000166893005\n",
      "Epoch 8, iter 201, loss 0.839596152305603, acc 0.7300000190734863\n",
      "Epoch 8, iter 301, loss 0.8829231262207031, acc 0.6899999976158142\n",
      "Epoch 8, iter 401, loss 0.9980305433273315, acc 0.6899999976158142\n",
      "Epoch 9, iter 1, loss 0.9885191917419434, acc 0.6700000166893005\n",
      "Epoch 9, iter 101, loss 0.8599382042884827, acc 0.699999988079071\n",
      "Epoch 9, iter 201, loss 0.8193575143814087, acc 0.7400000095367432\n",
      "Epoch 9, iter 301, loss 0.8648291826248169, acc 0.6899999976158142\n",
      "Epoch 9, iter 401, loss 0.9810851812362671, acc 0.6899999976158142\n",
      "Epoch 10, iter 1, loss 0.9708382487297058, acc 0.6700000166893005\n",
      "Epoch 10, iter 101, loss 0.841543972492218, acc 0.7099999785423279\n",
      "Epoch 10, iter 201, loss 0.8026599884033203, acc 0.7300000190734863\n",
      "Epoch 10, iter 301, loss 0.8492801189422607, acc 0.6899999976158142\n",
      "Epoch 10, iter 401, loss 0.9673361778259277, acc 0.6700000166893005\n"
     ]
    }
   ],
   "source": [
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=mod(x.float())\n",
    "        loss=CE(p,Y)\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aade743-989f-492d-a48f-d908c5f85be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can make one last change, instead of defining a loss function ourselves we will use a predifined one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d2d49d8-fde4-4f8f-8f13-4d74d552a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "030a9728-a6c5-4bb3-bbb4-521d4bef29ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7h/01sjg2sx6tl3y72r2klm24c80000gn/T/ipykernel_13130/2704172253.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 2.2954514026641846, acc 0.1599999964237213\n",
      "Epoch 1, iter 101, loss 2.2960641384124756, acc 0.15000000596046448\n",
      "Epoch 1, iter 201, loss 2.291412115097046, acc 0.11999999731779099\n",
      "Epoch 1, iter 301, loss 2.287562608718872, acc 0.11999999731779099\n",
      "Epoch 1, iter 401, loss 2.2863752841949463, acc 0.09000000357627869\n",
      "Epoch 2, iter 1, loss 2.2610960006713867, acc 0.1599999964237213\n",
      "Epoch 2, iter 101, loss 2.2560718059539795, acc 0.15000000596046448\n",
      "Epoch 2, iter 201, loss 2.252784013748169, acc 0.11999999731779099\n",
      "Epoch 2, iter 301, loss 2.2487235069274902, acc 0.11999999731779099\n",
      "Epoch 2, iter 401, loss 2.2575955390930176, acc 0.09000000357627869\n",
      "Epoch 3, iter 1, loss 2.212414503097534, acc 0.1599999964237213\n",
      "Epoch 3, iter 101, loss 2.219043254852295, acc 0.17000000178813934\n",
      "Epoch 3, iter 201, loss 2.227004051208496, acc 0.1599999964237213\n",
      "Epoch 3, iter 301, loss 2.229422092437744, acc 0.1899999976158142\n",
      "Epoch 3, iter 401, loss 2.2433786392211914, acc 0.17000000178813934\n",
      "Epoch 4, iter 1, loss 2.195537567138672, acc 0.25\n",
      "Epoch 4, iter 101, loss 2.2054078578948975, acc 0.20999999344348907\n",
      "Epoch 4, iter 201, loss 2.215907335281372, acc 0.20999999344348907\n",
      "Epoch 4, iter 301, loss 2.2203876972198486, acc 0.20000000298023224\n",
      "Epoch 4, iter 401, loss 2.235459566116333, acc 0.18000000715255737\n",
      "Epoch 5, iter 1, loss 2.1875863075256348, acc 0.27000001072883606\n",
      "Epoch 5, iter 101, loss 2.198206901550293, acc 0.2199999988079071\n",
      "Epoch 5, iter 201, loss 2.2094244956970215, acc 0.20999999344348907\n",
      "Epoch 5, iter 301, loss 2.2147669792175293, acc 0.20000000298023224\n",
      "Epoch 5, iter 401, loss 2.2301745414733887, acc 0.18000000715255737\n",
      "Epoch 6, iter 1, loss 2.1826422214508057, acc 0.2800000011920929\n",
      "Epoch 6, iter 101, loss 2.193574905395508, acc 0.2199999988079071\n",
      "Epoch 6, iter 201, loss 2.205024242401123, acc 0.2199999988079071\n",
      "Epoch 6, iter 301, loss 2.210725784301758, acc 0.20999999344348907\n",
      "Epoch 6, iter 401, loss 2.2262163162231445, acc 0.20000000298023224\n",
      "Epoch 7, iter 1, loss 2.1790273189544678, acc 0.3100000023841858\n",
      "Epoch 7, iter 101, loss 2.190206527709961, acc 0.2199999988079071\n",
      "Epoch 7, iter 201, loss 2.2017176151275635, acc 0.25\n",
      "Epoch 7, iter 301, loss 2.2074596881866455, acc 0.23000000417232513\n",
      "Epoch 7, iter 401, loss 2.2228691577911377, acc 0.25\n",
      "Epoch 8, iter 1, loss 2.1759839057922363, acc 0.3100000023841858\n",
      "Epoch 8, iter 101, loss 2.187431812286377, acc 0.2199999988079071\n",
      "Epoch 8, iter 201, loss 2.1989099979400635, acc 0.23999999463558197\n",
      "Epoch 8, iter 301, loss 2.204280138015747, acc 0.23000000417232513\n",
      "Epoch 8, iter 401, loss 2.219459295272827, acc 0.20999999344348907\n",
      "Epoch 9, iter 1, loss 2.1729280948638916, acc 0.23000000417232513\n",
      "Epoch 9, iter 101, loss 2.1846890449523926, acc 0.23000000417232513\n",
      "Epoch 9, iter 201, loss 2.195976972579956, acc 0.25999999046325684\n",
      "Epoch 9, iter 301, loss 2.1996963024139404, acc 0.25\n",
      "Epoch 9, iter 401, loss 2.2145698070526123, acc 0.2199999988079071\n",
      "Epoch 10, iter 1, loss 2.168506145477295, acc 0.25999999046325684\n",
      "Epoch 10, iter 101, loss 2.1809279918670654, acc 0.23999999463558197\n",
      "Epoch 10, iter 201, loss 2.191754102706909, acc 0.27000001072883606\n",
      "Epoch 10, iter 301, loss 2.189080238342285, acc 0.3100000023841858\n",
      "Epoch 10, iter 401, loss 2.2035775184631348, acc 0.28999999165534973\n"
     ]
    }
   ],
   "source": [
    "mod=MLP()\n",
    "opt=optim.SGD(mod.parameters(),lr=0.1)\n",
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=mod(x.float())\n",
    "        loss=criterion(p,Y.long())\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d772-9494-42f1-9977-77e06c3de6f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using keras and tensorflow to build the neural networks\n",
    "\n",
    "- Using tensors\n",
    "- Using Keras low level api\n",
    "- Using Keras Functional api\n",
    "- Using Keras Sequential api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bcc0af2-8889-4845-a36e-9b0a15a38ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7d81b-9967-4b08-af4f-42eb9dbb36b2",
   "metadata": {},
   "source": [
    "### Using tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39c3f952-399c-449c-9fc7-62390b7e3e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 17:43:54.384450: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-03 17:43:54.385017: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0580e968-7760-4518-a1eb-c7709ef47c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bias(k, initializer):\n",
    "    return tf.Variable(initializer(shape=[k,], dtype=tf.float32))\n",
    "def make_weights(n,k,initializer):\n",
    "    return tf.Variable(initializer(shape=[n,k], dtype=tf.float32))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3bc65a1-9d7a-47f9-a8bb-5331881a4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = make_weights(784,3,tf.random_normal_initializer())\n",
    "b1 = make_bias(3,tf.random_normal_initializer())\n",
    "w2 = make_weights(3,10,tf.random_normal_initializer())\n",
    "b2 = make_bias(10,tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af140368-13fd-44b8-9ff7-1e33319fb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(X,w1,b1,w2,b2):\n",
    "    z1=tf.matmul(X,w1)+b1\n",
    "    res1=tf.math.sigmoid(z1)\n",
    "    z2=tf.matmul(res1,w2)+b2\n",
    "    probs=tf.nn.softmax(z2,axis=1)\n",
    "    return probs\n",
    "CE = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e1d8596-ddf3-4246-8397-80084aa38dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward pass\n",
    "p=network(X,w1,b1,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88cd9697-7307-4a26-8d18-c1294e8baae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.3038237>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE(y,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c7d2951-e7c7-4e84-ac7a-05c75e2a51e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, loss 2.3025851249694824, acc 0\n",
      "Iter 2, loss 2.3025636672973633, acc 0\n",
      "Iter 3, loss 2.3025424480438232, acc 0\n",
      "Iter 4, loss 2.3025214672088623, acc 0\n",
      "Iter 5, loss 2.3025012016296387, acc 0\n",
      "Iter 6, loss 2.302480697631836, acc 0\n",
      "Iter 7, loss 2.3024609088897705, acc 0\n",
      "Iter 8, loss 2.302441120147705, acc 0\n",
      "Iter 9, loss 2.302421808242798, acc 0\n",
      "Iter 10, loss 2.3024024963378906, acc 0\n"
     ]
    }
   ],
   "source": [
    "## training loop\n",
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')\n",
    "\n",
    "w1 = make_weights(784,3,tf.zeros_initializer())\n",
    "b1 = make_bias(3,tf.zeros_initializer())\n",
    "w2 = make_weights(3,10,tf.zeros_initializer())\n",
    "b2 = make_bias(10,tf.zeros_initializer())\n",
    "\n",
    "lr=0.1\n",
    "Loss=[]\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        p=network(X,w1,b1,w2,b2)\n",
    "        loss = CE(y,p)\n",
    "    pred = tf.cast(tf.argmax(p,axis=1),tf.int32)\n",
    "    mask = tf.equal(pred,tf.cast(y,dtype = tf.int32))\n",
    "    mask = tf.cast(mask,dtype=tf.int32)\n",
    "    acc = tf.reduce_mean(mask)\n",
    "    print(f\"Iter {i+1}, loss {loss.numpy()}, acc {acc.numpy()}\")\n",
    "    ## update the weights\n",
    "    gw1,gb1,gw2,gb2 = tape.gradient(loss,[w1,b1,w2,b2])\n",
    "    w1.assign_sub(lr*gw1)\n",
    "    b1.assign_sub(lr*gb1)\n",
    "    w2.assign_sub(lr*gw2)\n",
    "    b2.assign_sub(lr*b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa953e92-659b-47de-9ee1-6a8d49eac6d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using keras-low-level api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "390a9f5c-559b-4c26-9d74-5b27fa3fedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a3cf67d-0f73-4b8d-90d3-5eb3737abb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.layers.Layer):\n",
    "    def __init__(self,layer_1_shape,layer_1_num_units, layer_2_num_units,layer_2_shape):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = Linear(units = layer_1_num_units,input_dim=layer_1_shape)\n",
    "        self.layer2 = Linear(units=layer_2_num_units,input_dim=layer_2_shape)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        x = tf.math.sigmoid(x)\n",
    "        x = self.layer2(x)\n",
    "        return tf.nn.softmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "176b0290-a1b1-4264-9205-c7ed75012554",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(784,3,10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c63b9e7-af79-4a4d-ae60-7292fd3b82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')\n",
    "mnist_data = tf.data.Dataset.from_tensor_slices((X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2df62d2f-e9a1-43c6-a3a0-9e390ce363b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = mnist_data.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e48871a-f380-41cf-8813-c1591875ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cf7e3a0-319a-4bf2-aeb2-1f694b978cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.3036980628967285 Accuracy: 0.0\n",
      "Step: 100 Loss: 2.2934751510620117 Accuracy: 0.0\n",
      "Step: 200 Loss: 2.2922370433807373 Accuracy: 0.0\n",
      "Step: 300 Loss: 2.294084310531616 Accuracy: 0.0\n",
      "Step: 400 Loss: 2.3088443279266357 Accuracy: 0.0\n",
      "Step: 500 Loss: 2.2955589294433594 Accuracy: 0.0\n",
      "Step: 600 Loss: 2.292743682861328 Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "for step, (x, y_1) in enumerate(mnist_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        probs = model(x)\n",
    "        pred = tf.cast(tf.argmax(probs,axis=1),tf.int32)\n",
    "        mask = tf.equal(pred,tf.cast(y_1,dtype=tf.int32))\n",
    "        mask = tf.cast(mask,dtype=tf.int32)\n",
    "        acc = tf.reduce_mean(mask)\n",
    "\n",
    "\n",
    "        # External loss value for this batch.\n",
    "        loss = loss_fn(y_1, probs)\n",
    "\n",
    "        # Add the losses created during the forward pass.\n",
    "        loss += sum(model.losses)\n",
    "\n",
    "        # Get gradients of the loss wrt the weights.\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss),'Accuracy:',float(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe4b04-cb50-47ab-bbe6-5ed96e7fd63d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using keras functional api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55770adb-9044-43a8-ba25-47faf3b464a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(784,))\n",
    "x=tf.keras.layers.Dense(units=3,activation=\"sigmoid\")(inputs)\n",
    "output = tf.keras.layers.Dense(units=10,activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b0df0b6-b628-4fa4-8212-f5cd4c584218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 2355      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,395\n",
      "Trainable params: 2,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a63da4a-4a09-4fb4-b31e-8c1d7c2760f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "700c003f-f297-4592-b0b3-d83263ee8e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 17:44:29.965764: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-07-03 17:44:29.970628: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657/657 [==============================] - 4s 6ms/step - loss: 2.2474 - accuracy: 0.1655\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 2.1404 - accuracy: 0.2854\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 2.0450 - accuracy: 0.3565\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 1.9499 - accuracy: 0.4401\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 1.8555 - accuracy: 0.5106\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 1.7641 - accuracy: 0.5622\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 1.6785 - accuracy: 0.5926\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 1.6006 - accuracy: 0.6129\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 1.5309 - accuracy: 0.6245\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 5s 7ms/step - loss: 1.4691 - accuracy: 0.6336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b7db4490>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mnist_data,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97255a80-8f09-431f-980e-96c51d2911c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Keras Sequential api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "412d216d-af5d-42be-9098-14b6e811625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "    tf.keras.layers.Dense(units=3,activation='sigmoid',input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(units=10,activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6cc23e5-19a9-4571-8265-78b6dda2c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "594ca7d2-1f69-47e9-9302-eeff955825c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  8/657 [..............................] - ETA: 4s - loss: 2.3391 - accuracy: 0.1250  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 17:45:14.414254: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657/657 [==============================] - 4s 6ms/step - loss: 2.2447 - accuracy: 0.1967\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 2.1381 - accuracy: 0.2881\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 2.0430 - accuracy: 0.3380\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 3s 5ms/step - loss: 1.9462 - accuracy: 0.4307\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 4s 5ms/step - loss: 1.8508 - accuracy: 0.5100\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 1.7602 - accuracy: 0.5545\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 3s 5ms/step - loss: 1.6766 - accuracy: 0.5816\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 3s 5ms/step - loss: 1.6008 - accuracy: 0.6008\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 3s 5ms/step - loss: 1.5328 - accuracy: 0.6130\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 3s 5ms/step - loss: 1.4722 - accuracy: 0.6219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b7bf50d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mnist_data,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c18cff-0ac7-4dc3-aecc-6e3880c9ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988d9c9-8aaa-4484-84fb-28089a8d2509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
