{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50eb9821-293e-438f-9563-98e52029ef21",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cbd10-637e-4b02-a97b-e98d361b002d",
   "metadata": {},
   "source": [
    "### Using autograd libraries to compute gradients\n",
    "\n",
    "- Implement Gradient Descent using auto-grad\n",
    "- Estimate Linear and Logistic Regression using auto-grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2634d0-361f-44ac-bd74-f2c0ff420e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80afe6a-7c53-4dd5-93cc-e13dbd18356c",
   "metadata": {},
   "source": [
    "Minmize $X^2+4X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38db9fe1-0670-4a62-b53f-c82e87dcda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ded0f74-a4f3-4c6c-98cf-2504da1769ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x ### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de08a36-eee3-4f57-98bf-08710f1c8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb3ad24-6dbf-4b93-8609-64dfe256ae0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### Grad of x at 0 wrt z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f793a8-94b6-4bb2-973c-ee0cbfc20036",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a125d92-5811-406b-a24f-2115c2cf0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02b4add-f334-4e16-99f1-508330519124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb1640e-775d-449c-997d-e044b15df5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83072cae-edc8-4d7b-81da-4b2a84633d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea003ef4-dc4c-48f9-a047-7a0eb638b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z 0.0, x: -0.03999999910593033\n",
      "Z -0.15839999914169312, x: -0.07919999957084656\n",
      "Z -0.31052735447883606, x: -0.11761599779129028\n",
      "Z -0.4566304683685303, x: -0.15526367723941803\n",
      "Z -0.5969479084014893, x: -0.19215840101242065\n",
      "Z -0.7317087650299072, x: -0.22831523418426514\n",
      "Z -0.8611330986022949, x: -0.2637489140033722\n",
      "Z -0.9854321479797363, x: -0.29847392439842224\n",
      "Z -1.104809045791626, x: -0.3325044512748718\n",
      "Z -1.2194585800170898, x: -0.3658543527126312\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(10):\n",
    "    z=x*x+4*x\n",
    "    z.backward() ### dz/dx\n",
    "    with torch.no_grad(): ##Disables any gradient computation\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51357e05-96d2-48e7-a45a-bcb2321f3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Autodiff\n",
    "### xy=750=>x=750/y\n",
    "## x+10y Minimize this\n",
    "### 750/y+y*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be388e75-807d-4313-9830-ce6f949af644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: 760.0, x: 8.399999618530273\n",
      "Z: 173.2857208251953, x: 8.406291961669922\n",
      "Z: 173.28179931640625, x: 8.41242504119873\n",
      "Z: 173.27809143066406, x: 8.418403625488281\n",
      "Z: 173.27456665039062, x: 8.42423152923584\n",
      "Z: 173.27120971679688, x: 8.429913520812988\n",
      "Z: 173.2680206298828, x: 8.435453414916992\n",
      "Z: 173.26498413085938, x: 8.4408540725708\n",
      "Z: 173.26211547851562, x: 8.446120262145996\n",
      "Z: 173.25936889648438, x: 8.451254844665527\n",
      "Z: 173.25677490234375, x: 8.45626163482666\n",
      "Z: 173.25428771972656, x: 8.46114444732666\n",
      "Z: 173.25193786621094, x: 8.465906143188477\n",
      "Z: 173.24969482421875, x: 8.470550537109375\n",
      "Z: 173.24755859375, x: 8.475079536437988\n",
      "Z: 173.24554443359375, x: 8.479496955871582\n",
      "Z: 173.2436065673828, x: 8.483805656433105\n",
      "Z: 173.2417755126953, x: 8.488008499145508\n",
      "Z: 173.24002075195312, x: 8.492108345031738\n",
      "Z: 173.23837280273438, x: 8.496108055114746\n",
      "Z: 173.23678588867188, x: 8.500009536743164\n",
      "Z: 173.23529052734375, x: 8.503815650939941\n",
      "Z: 173.23385620117188, x: 8.507528305053711\n",
      "Z: 173.2324981689453, x: 8.511151313781738\n",
      "Z: 173.231201171875, x: 8.51468563079834\n",
      "Z: 173.22998046875, x: 8.518134117126465\n",
      "Z: 173.22879028320312, x: 8.521498680114746\n",
      "Z: 173.22767639160156, x: 8.524782180786133\n",
      "Z: 173.22659301757812, x: 8.527985572814941\n",
      "Z: 173.2255859375, x: 8.531111717224121\n",
      "Z: 173.22463989257812, x: 8.534162521362305\n",
      "Z: 173.22372436523438, x: 8.537138938903809\n",
      "Z: 173.22283935546875, x: 8.540043830871582\n",
      "Z: 173.2220001220703, x: 8.542879104614258\n",
      "Z: 173.22120666503906, x: 8.545645713806152\n",
      "Z: 173.220458984375, x: 8.548345565795898\n",
      "Z: 173.2197265625, x: 8.550980567932129\n",
      "Z: 173.21905517578125, x: 8.553552627563477\n",
      "Z: 173.2183837890625, x: 8.556062698364258\n",
      "Z: 173.2177734375, x: 8.558512687683105\n",
      "Z: 173.21717834472656, x: 8.560904502868652\n",
      "Z: 173.21661376953125, x: 8.563239097595215\n",
      "Z: 173.216064453125, x: 8.56551742553711\n",
      "Z: 173.21556091308594, x: 8.567741394042969\n",
      "Z: 173.21507263183594, x: 8.569912910461426\n",
      "Z: 173.214599609375, x: 8.57203197479248\n",
      "Z: 173.21417236328125, x: 8.574100494384766\n",
      "Z: 173.2137451171875, x: 8.576120376586914\n",
      "Z: 173.21331787109375, x: 8.578091621398926\n",
      "Z: 173.21295166015625, x: 8.580016136169434\n",
      "Z: 173.21258544921875, x: 8.581894874572754\n",
      "Z: 173.2122344970703, x: 8.58372974395752\n",
      "Z: 173.2119140625, x: 8.58552074432373\n",
      "Z: 173.21157836914062, x: 8.587268829345703\n",
      "Z: 173.21127319335938, x: 8.58897590637207\n",
      "Z: 173.21099853515625, x: 8.590642929077148\n",
      "Z: 173.21072387695312, x: 8.592269897460938\n",
      "Z: 173.21044921875, x: 8.59385871887207\n",
      "Z: 173.210205078125, x: 8.595409393310547\n",
      "Z: 173.20997619628906, x: 8.596923828125\n",
      "Z: 173.20974731445312, x: 8.598402976989746\n",
      "Z: 173.20953369140625, x: 8.599846839904785\n",
      "Z: 173.20932006835938, x: 8.601256370544434\n",
      "Z: 173.20913696289062, x: 8.602632522583008\n",
      "Z: 173.2089385986328, x: 8.603976249694824\n",
      "Z: 173.20877075195312, x: 8.6052885055542\n",
      "Z: 173.20858764648438, x: 8.60657024383545\n",
      "Z: 173.20843505859375, x: 8.607821464538574\n",
      "Z: 173.20828247070312, x: 8.60904312133789\n",
      "Z: 173.2081298828125, x: 8.610236167907715\n",
      "Z: 173.20799255371094, x: 8.611401557922363\n",
      "Z: 173.20785522460938, x: 8.612539291381836\n",
      "Z: 173.20773315429688, x: 8.61365032196045\n",
      "Z: 173.20761108398438, x: 8.61473560333252\n",
      "Z: 173.20748901367188, x: 8.615795135498047\n",
      "Z: 173.20736694335938, x: 8.616829872131348\n",
      "Z: 173.207275390625, x: 8.617840766906738\n",
      "Z: 173.2071533203125, x: 8.618827819824219\n",
      "Z: 173.2070770263672, x: 8.619791030883789\n",
      "Z: 173.20697021484375, x: 8.620732307434082\n",
      "Z: 173.20687866210938, x: 8.621651649475098\n",
      "Z: 173.20680236816406, x: 8.622549057006836\n",
      "Z: 173.20672607421875, x: 8.623425483703613\n",
      "Z: 173.20664978027344, x: 8.624281883239746\n",
      "Z: 173.20657348632812, x: 8.625118255615234\n",
      "Z: 173.20651245117188, x: 8.625934600830078\n",
      "Z: 173.20645141601562, x: 8.626731872558594\n",
      "Z: 173.20639038085938, x: 8.627511024475098\n",
      "Z: 173.20632934570312, x: 8.628271102905273\n",
      "Z: 173.20626831054688, x: 8.629014015197754\n",
      "Z: 173.20620727539062, x: 8.629739761352539\n",
      "Z: 173.20614624023438, x: 8.630448341369629\n",
      "Z: 173.20611572265625, x: 8.63114070892334\n",
      "Z: 173.2060546875, x: 8.631816864013672\n",
      "Z: 173.20602416992188, x: 8.632476806640625\n",
      "Z: 173.2059783935547, x: 8.633121490478516\n",
      "Z: 173.2059326171875, x: 8.633750915527344\n",
      "Z: 173.20590209960938, x: 8.634366035461426\n",
      "Z: 173.2058563232422, x: 8.634966850280762\n",
      "Z: 173.20582580566406, x: 8.635553359985352\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(100):\n",
    "    z=(750/x)+x*10\n",
    "    z.backward()\n",
    "    with torch.no_grad():\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z: {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe0d2807-c717-464c-9834-b93088ae5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reg=pd.read_csv(\"../data/regression.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e804632d-e967-402a-869e-33ae686ec84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0  70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5  70.0   \n",
       "\n",
       "   origin  \n",
       "0     1.0  \n",
       "1     1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da66b63-27f0-4cce-8fca-288df8cab9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### mpg=b0+b1*cyl ## as a matrix product?\n",
    "### dloss/db0, dloss/db1\n",
    "#### loss=f(b0,b1)\n",
    "##loss=eq\n",
    "##loss.backward()\n",
    "##b0.grad\n",
    "##b1.grad\n",
    "X=reg[['cylinders']].values\n",
    "y=reg[['mpg']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36051e23-3124-4a1c-8c50-81d783b4673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## W dim?=> dloss/dW,dloss/db\n",
    "W=torch.randn(1,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f3d9b0b-bb22-4050-832f-796d321078a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85019d7d-bc7c-4bc2-8574-e7cd23693d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 656.742904087977, W: [[2.4741]], b: [0.52897274]\n",
      "Loss: 238.55183099785884, W: [[3.2661462]], b: [0.3202374]\n",
      "Loss: 193.9184689687916, W: [[3.5150633]], b: [0.20235732]\n",
      "Loss: 188.70296726041, W: [[3.5875692]], b: [0.11407603]\n",
      "Loss: 187.64840846052513, W: [[3.6027856]], b: [0.03549533]\n",
      "Loss: 187.03462827013078, W: [[3.5994065]], b: [-0.03984849]\n",
      "Loss: 186.46908242105295, W: [[3.5900018]], b: [-0.11405525]\n",
      "Loss: 185.9103467995149, W: [[3.5786538]], b: [-0.18780711]\n",
      "Loss: 185.3540457445425, W: [[3.566689]], b: [-0.26132584]\n",
      "Loss: 184.7997097647657, W: [[3.5545375]], b: [-0.3346836]\n",
      "Loss: 184.2472826740042, W: [[3.5423403]], b: [-0.40790406]\n",
      "Loss: 183.69675593192903, W: [[3.5301418]], b: [-0.48099497]\n",
      "Loss: 183.14811985990613, W: [[3.5179574]], b: [-0.553959]\n",
      "Loss: 182.6013690909681, W: [[3.5057921]], b: [-0.62679726]\n",
      "Loss: 182.05649682534835, W: [[3.4936466]], b: [-0.6995101]\n",
      "Loss: 181.51349766162096, W: [[3.4815216]], b: [-0.7720978]\n",
      "Loss: 180.9723636998647, W: [[3.4694176]], b: [-0.84456074]\n",
      "Loss: 180.4330892050776, W: [[3.457334]], b: [-0.9168991]\n",
      "Loss: 179.89566642583875, W: [[3.4452717]], b: [-0.9891131]\n",
      "Loss: 179.36009203235568, W: [[3.43323]], b: [-1.0612029]\n",
      "Loss: 178.82635723970535, W: [[3.4212089]], b: [-1.1331687]\n",
      "Loss: 178.29445488585532, W: [[3.4092083]], b: [-1.2050108]\n",
      "Loss: 177.76438186740683, W: [[3.3972285]], b: [-1.2767293]\n",
      "Loss: 177.23612961169084, W: [[3.3852694]], b: [-1.3483245]\n",
      "Loss: 176.70969283529348, W: [[3.3733306]], b: [-1.4197967]\n",
      "Loss: 176.18506307328366, W: [[3.3614123]], b: [-1.491146]\n",
      "Loss: 175.66223767040748, W: [[3.3495147]], b: [-1.5623726]\n",
      "Loss: 175.14120746676073, W: [[3.3376374]], b: [-1.6334767]\n",
      "Loss: 174.62196847281282, W: [[3.3257809]], b: [-1.7044586]\n",
      "Loss: 174.10451262562876, W: [[3.3139443]], b: [-1.7753184]\n",
      "Loss: 173.58883575952615, W: [[3.3021286]], b: [-1.8460563]\n",
      "Loss: 173.07492947810871, W: [[3.2903326]], b: [-1.9166727]\n",
      "Loss: 172.5627891794891, W: [[3.2785573]], b: [-1.9871676]\n",
      "Loss: 172.05241020303833, W: [[3.266802]], b: [-2.0575414]\n",
      "Loss: 171.54378260293558, W: [[3.2550673]], b: [-2.127794]\n",
      "Loss: 171.03690334175758, W: [[3.2433527]], b: [-2.197926]\n",
      "Loss: 170.53176573254603, W: [[3.2316582]], b: [-2.2679374]\n",
      "Loss: 170.02836431557688, W: [[3.2199833]], b: [-2.3378284]\n",
      "Loss: 169.5266915138602, W: [[3.2083292]], b: [-2.4075992]\n",
      "Loss: 169.02674292537372, W: [[3.1966946]], b: [-2.4772499]\n",
      "Loss: 168.52851331917358, W: [[3.1850803]], b: [-2.5467808]\n",
      "Loss: 168.031994514559, W: [[3.1734858]], b: [-2.6161923]\n",
      "Loss: 167.53718175109472, W: [[3.1619112]], b: [-2.6854844]\n",
      "Loss: 167.04406991537144, W: [[3.1503565]], b: [-2.7546573]\n",
      "Loss: 166.55265236537434, W: [[3.138822]], b: [-2.8237114]\n",
      "Loss: 166.0629213278239, W: [[3.1273072]], b: [-2.8926468]\n",
      "Loss: 165.57487461623876, W: [[3.1158118]], b: [-2.9614635]\n",
      "Loss: 165.0885039111524, W: [[3.1043367]], b: [-3.0301619]\n",
      "Loss: 164.6038060815592, W: [[3.0928812]], b: [-3.0987422]\n",
      "Loss: 164.1207720433313, W: [[3.0814452]], b: [-3.1672046]\n",
      "Loss: 163.63939750540823, W: [[3.0700288]], b: [-3.2355492]\n",
      "Loss: 163.1596792158288, W: [[3.0586324]], b: [-3.3037765]\n",
      "Loss: 162.68160687981515, W: [[3.0472553]], b: [-3.3718863]\n",
      "Loss: 162.20517739141417, W: [[3.0358977]], b: [-3.439879]\n",
      "Loss: 161.7303852035262, W: [[3.0245597]], b: [-3.5077548]\n",
      "Loss: 161.2572244809278, W: [[3.0132415]], b: [-3.5755138]\n",
      "Loss: 160.78569029060057, W: [[3.0019424]], b: [-3.6431565]\n",
      "Loss: 160.3157747396091, W: [[2.990663]], b: [-3.7106829]\n",
      "Loss: 159.8474741295768, W: [[2.979403]], b: [-3.778093]\n",
      "Loss: 159.38078415025652, W: [[2.9681618]], b: [-3.8453875]\n",
      "Loss: 158.9156951853591, W: [[2.9569407]], b: [-3.9125662]\n",
      "Loss: 158.45220450473377, W: [[2.945738]], b: [-3.9796293]\n",
      "Loss: 157.99030705234546, W: [[2.9345553]], b: [-4.046577]\n",
      "Loss: 157.5299974583295, W: [[2.9233916]], b: [-4.1134095]\n",
      "Loss: 157.07126956897048, W: [[2.9122472]], b: [-4.180127]\n",
      "Loss: 156.61411831540343, W: [[2.9011219]], b: [-4.2467303]\n",
      "Loss: 156.15853452834654, W: [[2.8900156]], b: [-4.313219]\n",
      "Loss: 155.7045168357308, W: [[2.8789287]], b: [-4.3795934]\n",
      "Loss: 155.25205913120695, W: [[2.8678606]], b: [-4.445853]\n",
      "Loss: 154.80115886219122, W: [[2.8568115]], b: [-4.5119996]\n",
      "Loss: 154.351804973986, W: [[2.8457818]], b: [-4.578032]\n",
      "Loss: 153.90399722558058, W: [[2.8347707]], b: [-4.643951]\n",
      "Loss: 153.45772669880344, W: [[2.8237784]], b: [-4.7097564]\n",
      "Loss: 153.01299002452254, W: [[2.8128052]], b: [-4.775449]\n",
      "Loss: 152.56978071191543, W: [[2.8018508]], b: [-4.841028]\n",
      "Loss: 152.1280942283032, W: [[2.7909155]], b: [-4.906495]\n",
      "Loss: 151.68792546099147, W: [[2.7799988]], b: [-4.971849]\n",
      "Loss: 151.24927108644658, W: [[2.7691007]], b: [-5.037091]\n",
      "Loss: 150.81212231750214, W: [[2.7582214]], b: [-5.1022205]\n",
      "Loss: 150.37647319392116, W: [[2.747361]], b: [-5.167238]\n",
      "Loss: 149.94232252362067, W: [[2.736519]], b: [-5.232144]\n",
      "Loss: 149.50966585062957, W: [[2.725696]], b: [-5.296938]\n",
      "Loss: 149.07849414715284, W: [[2.7148917]], b: [-5.361621]\n",
      "Loss: 148.6488039286328, W: [[2.7041056]], b: [-5.4261923]\n",
      "Loss: 148.22059118524683, W: [[2.6933384]], b: [-5.4906526]\n",
      "Loss: 147.79385021247228, W: [[2.6825893]], b: [-5.555002]\n",
      "Loss: 147.36857371600854, W: [[2.6718588]], b: [-5.619241]\n",
      "Loss: 146.9447591052491, W: [[2.6611469]], b: [-5.6833696]\n",
      "Loss: 146.5224012693748, W: [[2.650453]], b: [-5.747388]\n",
      "Loss: 146.10149417116773, W: [[2.639778]], b: [-5.811296]\n",
      "Loss: 145.68203342179, W: [[2.6291213]], b: [-5.8750944]\n",
      "Loss: 145.26401334981864, W: [[2.6184828]], b: [-5.938783]\n",
      "Loss: 144.84742930969475, W: [[2.6078627]], b: [-6.0023623]\n",
      "Loss: 144.43227733287517, W: [[2.5972607]], b: [-6.065832]\n",
      "Loss: 144.0185508451921, W: [[2.586677]], b: [-6.129193]\n",
      "Loss: 143.60624602753188, W: [[2.5761118]], b: [-6.192445]\n",
      "Loss: 143.19535705417428, W: [[2.5655642]], b: [-6.2555876]\n",
      "Loss: 142.7858820124466, W: [[2.5550346]], b: [-6.318622]\n",
      "Loss: 142.37781181047984, W: [[2.5445237]], b: [-6.3815484]\n",
      "Loss: 141.97114339098994, W: [[2.534031]], b: [-6.4443665]\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "for i in range(100):\n",
    "    diff=y-torch.matmul(X.float(),W)+b\n",
    "    loss=sum(diff*diff)/y.shape[0]\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W-=lr*W.grad\n",
    "        b-=lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}, W: {W.detach().numpy()}, b: {b.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9367d643-03ff-4e2a-859f-63211d040f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### linear classifier.\n",
    "### Can you estimate a linear classifier using autodiff\n",
    "### Loss for linear classifier?\n",
    "### log loss as a function of W and b, p=f(X,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c648fb49-1a49-4f98-812f-91c57b25f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_pregnant</th>\n",
       "      <th>Plasma_glucose</th>\n",
       "      <th>Blood_pres</th>\n",
       "      <th>Skin_thick</th>\n",
       "      <th>Serum_insu</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Diabetes_func</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No_pregnant  Plasma_glucose  Blood_pres  Skin_thick  Serum_insu   BMI  \\\n",
       "0            6             148          72          35           0  33.6   \n",
       "1            1              85          66          29           0  26.6   \n",
       "2            8             183          64           0           0  23.3   \n",
       "3            1              89          66          23          94  28.1   \n",
       "4            0             137          40          35         168  43.1   \n",
       "\n",
       "   Diabetes_func  Age  Class  \n",
       "0          0.627   50      1  \n",
       "1          0.351   31      0  \n",
       "2          0.672   32      1  \n",
       "3          0.167   21      0  \n",
       "4          2.288   33      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls=pd.read_csv(\"../data/classification.csv\")\n",
    "cls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1496d643-bfa1-4ee4-a443-4930d5bca9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cls[['No_pregnant']].values\n",
    "y=cls[['Class']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54f17e9-a211-4e1f-a63e-2fbad4bedf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss=−[𝑦𝑙𝑜𝑔(𝑝+tol)+(1−𝑦)𝑙𝑜𝑔(1−𝑝+tol)]\n",
    "### p=1/(1+e^-z)\n",
    "### z= XW+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d76d581-af74-4ca0-b5f9-7cf84d9b6c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8471252918243408, W: tensor([[0.2586]], requires_grad=True), b: tensor([-0.3354], requires_grad=True)\n",
      "Loss: 0.8292115330696106, W: tensor([[0.2459]], requires_grad=True), b: tensor([-0.3382], requires_grad=True)\n",
      "Loss: 0.8124645352363586, W: tensor([[0.2336]], requires_grad=True), b: tensor([-0.3409], requires_grad=True)\n",
      "Loss: 0.7968730926513672, W: tensor([[0.2217]], requires_grad=True), b: tensor([-0.3436], requires_grad=True)\n",
      "Loss: 0.7824189066886902, W: tensor([[0.2104]], requires_grad=True), b: tensor([-0.3462], requires_grad=True)\n",
      "Loss: 0.769075334072113, W: tensor([[0.1994]], requires_grad=True), b: tensor([-0.3487], requires_grad=True)\n",
      "Loss: 0.7568092346191406, W: tensor([[0.1890]], requires_grad=True), b: tensor([-0.3511], requires_grad=True)\n",
      "Loss: 0.7455804944038391, W: tensor([[0.1790]], requires_grad=True), b: tensor([-0.3534], requires_grad=True)\n",
      "Loss: 0.7353433966636658, W: tensor([[0.1695]], requires_grad=True), b: tensor([-0.3557], requires_grad=True)\n",
      "Loss: 0.7260470986366272, W: tensor([[0.1605]], requires_grad=True), b: tensor([-0.3578], requires_grad=True)\n",
      "Loss: 0.7176375389099121, W: tensor([[0.1519]], requires_grad=True), b: tensor([-0.3599], requires_grad=True)\n",
      "Loss: 0.7100570797920227, W: tensor([[0.1438]], requires_grad=True), b: tensor([-0.3619], requires_grad=True)\n",
      "Loss: 0.7032477259635925, W: tensor([[0.1361]], requires_grad=True), b: tensor([-0.3639], requires_grad=True)\n",
      "Loss: 0.697149932384491, W: tensor([[0.1289]], requires_grad=True), b: tensor([-0.3657], requires_grad=True)\n",
      "Loss: 0.6917054653167725, W: tensor([[0.1221]], requires_grad=True), b: tensor([-0.3676], requires_grad=True)\n",
      "Loss: 0.6868570446968079, W: tensor([[0.1156]], requires_grad=True), b: tensor([-0.3693], requires_grad=True)\n",
      "Loss: 0.6825498938560486, W: tensor([[0.1096]], requires_grad=True), b: tensor([-0.3710], requires_grad=True)\n",
      "Loss: 0.6787314414978027, W: tensor([[0.1039]], requires_grad=True), b: tensor([-0.3726], requires_grad=True)\n",
      "Loss: 0.6753528118133545, W: tensor([[0.0986]], requires_grad=True), b: tensor([-0.3742], requires_grad=True)\n",
      "Loss: 0.6723679900169373, W: tensor([[0.0936]], requires_grad=True), b: tensor([-0.3757], requires_grad=True)\n",
      "Loss: 0.6697344779968262, W: tensor([[0.0889]], requires_grad=True), b: tensor([-0.3772], requires_grad=True)\n",
      "Loss: 0.6674137115478516, W: tensor([[0.0846]], requires_grad=True), b: tensor([-0.3786], requires_grad=True)\n",
      "Loss: 0.6653700470924377, W: tensor([[0.0805]], requires_grad=True), b: tensor([-0.3800], requires_grad=True)\n",
      "Loss: 0.663571834564209, W: tensor([[0.0767]], requires_grad=True), b: tensor([-0.3813], requires_grad=True)\n",
      "Loss: 0.6619901061058044, W: tensor([[0.0731]], requires_grad=True), b: tensor([-0.3826], requires_grad=True)\n",
      "Loss: 0.6605991125106812, W: tensor([[0.0698]], requires_grad=True), b: tensor([-0.3838], requires_grad=True)\n",
      "Loss: 0.659375786781311, W: tensor([[0.0667]], requires_grad=True), b: tensor([-0.3851], requires_grad=True)\n",
      "Loss: 0.6583000421524048, W: tensor([[0.0638]], requires_grad=True), b: tensor([-0.3863], requires_grad=True)\n",
      "Loss: 0.6573532223701477, W: tensor([[0.0611]], requires_grad=True), b: tensor([-0.3874], requires_grad=True)\n",
      "Loss: 0.6565197110176086, W: tensor([[0.0586]], requires_grad=True), b: tensor([-0.3885], requires_grad=True)\n",
      "Loss: 0.6557852625846863, W: tensor([[0.0563]], requires_grad=True), b: tensor([-0.3897], requires_grad=True)\n",
      "Loss: 0.6551374793052673, W: tensor([[0.0541]], requires_grad=True), b: tensor([-0.3907], requires_grad=True)\n",
      "Loss: 0.6545653939247131, W: tensor([[0.0521]], requires_grad=True), b: tensor([-0.3918], requires_grad=True)\n",
      "Loss: 0.6540594696998596, W: tensor([[0.0502]], requires_grad=True), b: tensor([-0.3928], requires_grad=True)\n",
      "Loss: 0.6536112427711487, W: tensor([[0.0484]], requires_grad=True), b: tensor([-0.3938], requires_grad=True)\n",
      "Loss: 0.6532135605812073, W: tensor([[0.0468]], requires_grad=True), b: tensor([-0.3948], requires_grad=True)\n",
      "Loss: 0.6528598666191101, W: tensor([[0.0453]], requires_grad=True), b: tensor([-0.3958], requires_grad=True)\n",
      "Loss: 0.6525445580482483, W: tensor([[0.0439]], requires_grad=True), b: tensor([-0.3968], requires_grad=True)\n",
      "Loss: 0.6522626280784607, W: tensor([[0.0426]], requires_grad=True), b: tensor([-0.3977], requires_grad=True)\n",
      "Loss: 0.6520100235939026, W: tensor([[0.0414]], requires_grad=True), b: tensor([-0.3987], requires_grad=True)\n",
      "Loss: 0.6517829298973083, W: tensor([[0.0403]], requires_grad=True), b: tensor([-0.3996], requires_grad=True)\n",
      "Loss: 0.651577889919281, W: tensor([[0.0392]], requires_grad=True), b: tensor([-0.4005], requires_grad=True)\n",
      "Loss: 0.65139240026474, W: tensor([[0.0383]], requires_grad=True), b: tensor([-0.4014], requires_grad=True)\n",
      "Loss: 0.6512235999107361, W: tensor([[0.0374]], requires_grad=True), b: tensor([-0.4022], requires_grad=True)\n",
      "Loss: 0.651069700717926, W: tensor([[0.0365]], requires_grad=True), b: tensor([-0.4031], requires_grad=True)\n",
      "Loss: 0.6509286165237427, W: tensor([[0.0358]], requires_grad=True), b: tensor([-0.4040], requires_grad=True)\n",
      "Loss: 0.6507988572120667, W: tensor([[0.0351]], requires_grad=True), b: tensor([-0.4048], requires_grad=True)\n",
      "Loss: 0.6506787538528442, W: tensor([[0.0344]], requires_grad=True), b: tensor([-0.4057], requires_grad=True)\n",
      "Loss: 0.6505672335624695, W: tensor([[0.0338]], requires_grad=True), b: tensor([-0.4065], requires_grad=True)\n",
      "Loss: 0.6504632830619812, W: tensor([[0.0332]], requires_grad=True), b: tensor([-0.4073], requires_grad=True)\n",
      "Loss: 0.6503656506538391, W: tensor([[0.0327]], requires_grad=True), b: tensor([-0.4081], requires_grad=True)\n",
      "Loss: 0.6502739191055298, W: tensor([[0.0323]], requires_grad=True), b: tensor([-0.4090], requires_grad=True)\n",
      "Loss: 0.650187075138092, W: tensor([[0.0318]], requires_grad=True), b: tensor([-0.4098], requires_grad=True)\n",
      "Loss: 0.6501045823097229, W: tensor([[0.0314]], requires_grad=True), b: tensor([-0.4106], requires_grad=True)\n",
      "Loss: 0.6500259637832642, W: tensor([[0.0311]], requires_grad=True), b: tensor([-0.4114], requires_grad=True)\n",
      "Loss: 0.6499506235122681, W: tensor([[0.0307]], requires_grad=True), b: tensor([-0.4121], requires_grad=True)\n",
      "Loss: 0.649878203868866, W: tensor([[0.0304]], requires_grad=True), b: tensor([-0.4129], requires_grad=True)\n",
      "Loss: 0.6498083472251892, W: tensor([[0.0302]], requires_grad=True), b: tensor([-0.4137], requires_grad=True)\n",
      "Loss: 0.6497407555580139, W: tensor([[0.0299]], requires_grad=True), b: tensor([-0.4145], requires_grad=True)\n",
      "Loss: 0.6496750712394714, W: tensor([[0.0297]], requires_grad=True), b: tensor([-0.4152], requires_grad=True)\n",
      "Loss: 0.6496111750602722, W: tensor([[0.0295]], requires_grad=True), b: tensor([-0.4160], requires_grad=True)\n",
      "Loss: 0.6495488286018372, W: tensor([[0.0293]], requires_grad=True), b: tensor([-0.4168], requires_grad=True)\n",
      "Loss: 0.6494877934455872, W: tensor([[0.0291]], requires_grad=True), b: tensor([-0.4175], requires_grad=True)\n",
      "Loss: 0.6494279503822327, W: tensor([[0.0290]], requires_grad=True), b: tensor([-0.4183], requires_grad=True)\n",
      "Loss: 0.6493691802024841, W: tensor([[0.0288]], requires_grad=True), b: tensor([-0.4190], requires_grad=True)\n",
      "Loss: 0.6493113040924072, W: tensor([[0.0287]], requires_grad=True), b: tensor([-0.4198], requires_grad=True)\n",
      "Loss: 0.6492542624473572, W: tensor([[0.0286]], requires_grad=True), b: tensor([-0.4205], requires_grad=True)\n",
      "Loss: 0.6491978168487549, W: tensor([[0.0285]], requires_grad=True), b: tensor([-0.4213], requires_grad=True)\n",
      "Loss: 0.6491422057151794, W: tensor([[0.0285]], requires_grad=True), b: tensor([-0.4220], requires_grad=True)\n",
      "Loss: 0.649087131023407, W: tensor([[0.0284]], requires_grad=True), b: tensor([-0.4227], requires_grad=True)\n",
      "Loss: 0.6490325331687927, W: tensor([[0.0284]], requires_grad=True), b: tensor([-0.4235], requires_grad=True)\n",
      "Loss: 0.6489784121513367, W: tensor([[0.0283]], requires_grad=True), b: tensor([-0.4242], requires_grad=True)\n",
      "Loss: 0.648924708366394, W: tensor([[0.0283]], requires_grad=True), b: tensor([-0.4249], requires_grad=True)\n",
      "Loss: 0.6488714218139648, W: tensor([[0.0283]], requires_grad=True), b: tensor([-0.4257], requires_grad=True)\n",
      "Loss: 0.6488184332847595, W: tensor([[0.0282]], requires_grad=True), b: tensor([-0.4264], requires_grad=True)\n",
      "Loss: 0.6487658023834229, W: tensor([[0.0282]], requires_grad=True), b: tensor([-0.4271], requires_grad=True)\n",
      "Loss: 0.6487134099006653, W: tensor([[0.0282]], requires_grad=True), b: tensor([-0.4278], requires_grad=True)\n",
      "Loss: 0.6486613154411316, W: tensor([[0.0282]], requires_grad=True), b: tensor([-0.4286], requires_grad=True)\n",
      "Loss: 0.648609459400177, W: tensor([[0.0283]], requires_grad=True), b: tensor([-0.4293], requires_grad=True)\n",
      "Loss: 0.6485578417778015, W: tensor([[0.0283]], requires_grad=True), b: tensor([-0.4300], requires_grad=True)\n",
      "Loss: 0.6485063433647156, W: tensor([[0.0283]], requires_grad=True), b: tensor([-0.4307], requires_grad=True)\n",
      "Loss: 0.6484551429748535, W: tensor([[0.0283]], requires_grad=True), b: tensor([-0.4314], requires_grad=True)\n",
      "Loss: 0.6484041213989258, W: tensor([[0.0284]], requires_grad=True), b: tensor([-0.4321], requires_grad=True)\n",
      "Loss: 0.6483532786369324, W: tensor([[0.0284]], requires_grad=True), b: tensor([-0.4328], requires_grad=True)\n",
      "Loss: 0.6483024954795837, W: tensor([[0.0284]], requires_grad=True), b: tensor([-0.4336], requires_grad=True)\n",
      "Loss: 0.6482519507408142, W: tensor([[0.0285]], requires_grad=True), b: tensor([-0.4343], requires_grad=True)\n",
      "Loss: 0.648201584815979, W: tensor([[0.0285]], requires_grad=True), b: tensor([-0.4350], requires_grad=True)\n",
      "Loss: 0.6481512784957886, W: tensor([[0.0286]], requires_grad=True), b: tensor([-0.4357], requires_grad=True)\n",
      "Loss: 0.6481011509895325, W: tensor([[0.0287]], requires_grad=True), b: tensor([-0.4364], requires_grad=True)\n",
      "Loss: 0.6480512022972107, W: tensor([[0.0287]], requires_grad=True), b: tensor([-0.4371], requires_grad=True)\n",
      "Loss: 0.6480012536048889, W: tensor([[0.0288]], requires_grad=True), b: tensor([-0.4378], requires_grad=True)\n",
      "Loss: 0.6479515433311462, W: tensor([[0.0288]], requires_grad=True), b: tensor([-0.4385], requires_grad=True)\n",
      "Loss: 0.6479019522666931, W: tensor([[0.0289]], requires_grad=True), b: tensor([-0.4392], requires_grad=True)\n",
      "Loss: 0.6478524804115295, W: tensor([[0.0290]], requires_grad=True), b: tensor([-0.4399], requires_grad=True)\n",
      "Loss: 0.6478030681610107, W: tensor([[0.0291]], requires_grad=True), b: tensor([-0.4406], requires_grad=True)\n",
      "Loss: 0.6477538347244263, W: tensor([[0.0291]], requires_grad=True), b: tensor([-0.4413], requires_grad=True)\n",
      "Loss: 0.6477046608924866, W: tensor([[0.0292]], requires_grad=True), b: tensor([-0.4420], requires_grad=True)\n",
      "Loss: 0.6476556658744812, W: tensor([[0.0293]], requires_grad=True), b: tensor([-0.4427], requires_grad=True)\n",
      "Loss: 0.6476066708564758, W: tensor([[0.0294]], requires_grad=True), b: tensor([-0.4434], requires_grad=True)\n",
      "Loss: 0.6475578546524048, W: tensor([[0.0294]], requires_grad=True), b: tensor([-0.4441], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)\n",
    "W=torch.randn(1,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)\n",
    "tol=0.0000000001\n",
    "lr=0.01\n",
    "for i in range(100):\n",
    "    z=torch.matmul(X.float(),W)+b\n",
    "    p=1.0/(1+torch.exp(-z))\n",
    "    loss=-(y*torch.log(p+tol)+(1-y)*torch.log(1-p+tol)).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W-=lr*W.grad\n",
    "        b-=lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}, W: {W}, b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d784aed-c43c-4a60-8ebe-e278937225be",
   "metadata": {},
   "source": [
    "## Using tensorflow to compute gradients \n",
    "\n",
    "$y = x^2 +4x$\n",
    "\n",
    "$\\frac{dy}{dx} = 2x+4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09d53aa3-c017-4833-97b3-4c363fd39ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 17:37:56.230587: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-03 17:37:56.230773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f13e850-0e34-42e3-9dd9-f12c9eeadcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = x**2+4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dce69b9-2b75-4275-847d-483ae3f3689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = tape.gradient(y, x) ## compute dy/dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7353647-f5a5-48c4-af01-150b62d76f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "639a3595-5117-4fcb-bf0a-6b3b208da5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e7824-7077-415f-b4e6-aa99b3fe0ede",
   "metadata": {},
   "source": [
    "Write the gradient descent using ```GradientTape()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ae3ed39-acf1-4aa4-a8d9-d7fe7db03578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4\n",
      "-0.72\n",
      "-0.9760001\n",
      "-1.1808001\n",
      "-1.34464\n",
      "-1.4757121\n",
      "-1.5805696\n",
      "-1.6644558\n",
      "-1.7315646\n",
      "-1.7852517\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.0)\n",
    "lr = 0.1\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x**2+4*x\n",
    "    grad = tape.gradient(y,x)\n",
    "    x.assign_sub(lr*grad)\n",
    "    print(x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943b60d-7903-48fe-bd12-edbfda48109b",
   "metadata": {},
   "source": [
    "### Linear Regression with basic tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "551a7a86-463a-4fe6-ada4-f15e9a9c2d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0  70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5  70.0   \n",
       "2  18.0        8.0         318.0       150.0  3436.0          11.0  70.0   \n",
       "3  16.0        8.0         304.0       150.0  3433.0          12.0  70.0   \n",
       "4  17.0        8.0         302.0       140.0  3449.0          10.5  70.0   \n",
       "\n",
       "   origin  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "778cf3c2-0785-4b6c-aa84-4c76f032be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=reg[['cylinders']].values\n",
    "y=reg[['mpg']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37815f60-cb6f-4d8b-9851-218d2e9259ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e11b8769-4f33-4300-94bf-9284b30cc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred,y):\n",
    "    return tf.reduce_mean(tf.square(y-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e24ce4c6-0502-447f-92e4-87df4d2fe2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(192.6729, shape=(), dtype=float32)\n",
      "tf.Tensor(115.464874, shape=(), dtype=float32)\n",
      "tf.Tensor(107.17809, shape=(), dtype=float32)\n",
      "tf.Tensor(106.16406, shape=(), dtype=float32)\n",
      "tf.Tensor(105.9179, shape=(), dtype=float32)\n",
      "tf.Tensor(105.75324, shape=(), dtype=float32)\n",
      "tf.Tensor(105.59764, shape=(), dtype=float32)\n",
      "tf.Tensor(105.4435, shape=(), dtype=float32)\n",
      "tf.Tensor(105.289955, shape=(), dtype=float32)\n",
      "tf.Tensor(105.13697, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random.normal(shape=(1,1),dtype='float32'))\n",
    "B = tf.Variable(tf.random.normal(shape=(1,),dtype='float32'))\n",
    "lr = 0.01\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.reshape(X@W+B,shape=X.shape[0])\n",
    "        error = loss(y_pred,y)\n",
    "    dw,db = tape.gradient(error,[W,B])\n",
    "    W.assign_sub(lr*dw)\n",
    "    B.assign_sub(lr*db)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5cbf9a4-98e9-4bb8-8a0e-15bcb3e1aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class Excercise: Use tensorflow api to write the gradients for logistic regresssion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
